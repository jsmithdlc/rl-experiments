{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub\n",
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Environment\n",
    "\n",
    "Understanding how Gymnasium library works. It's useful for its environments in which RL can be tested out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "Observation Space Shape (8,)\n",
      "Sample observation [-5.9849072e+01  5.7347160e+01  4.5207868e+00  1.8485782e+00\n",
      " -4.9732834e-02 -1.3853170e+00  1.3320468e-01  8.0533069e-01]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# create environment\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "env.reset()\n",
    "\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Sample observation\", env.observation_space.sample())  # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "Action Space Shape 4\n",
      "Action Space Sample 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample())  # Take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken:  2\n",
      "Action taken:  1\n",
      "Action taken:  1\n",
      "Action taken:  3\n",
      "Action taken:  2\n",
      "Action taken:  1\n",
      "Action taken:  2\n",
      "Action taken:  1\n",
      "Action taken:  1\n",
      "Action taken:  3\n",
      "Action taken:  1\n",
      "Action taken:  1\n",
      "Action taken:  3\n",
      "Action taken:  3\n",
      "Action taken:  3\n",
      "Action taken:  1\n",
      "Action taken:  3\n",
      "Action taken:  1\n",
      "Action taken:  0\n",
      "Action taken:  2\n"
     ]
    }
   ],
   "source": [
    "# reset environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(20):\n",
    "    # take random action\n",
    "    action = env.action_space.sample()\n",
    "    print(\"Action taken: \",action)\n",
    "\n",
    "    # perform this action in environment and retrieve info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # if the game terminated, stop environment\n",
    "    if terminated or truncated:\n",
    "        # reset env\n",
    "        print(\"Environment reset\")\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vectorized environment, where we can stack multiple independent environments into a single one. This way we can create more diverse experiences during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = make_vec_env(\"LunarLander-v2\", n_envs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Using StableBaselines3, PPO will be our model. PPO combines value-based RL and policy-based RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# instantiate agent\n",
    "model = PPO('MlpPolicy',env,n_steps=1024,batch_size=64,n_epochs=4,gamma=0.999,gae_lambda=0.98,ent_coef=0.01,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-experiments-r-JsmOJR-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
